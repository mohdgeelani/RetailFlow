# RetailFlow - End-to-End Retail Data Pipeline (Fully Automated & Containerized)

RetailFlow is a fully automated and containerized **ETL data pipeline** for retail sales data â€” from ingestion to transformation, loading, and interactive reporting â€” built using industry-grade tools like **Airflow**, **Docker**, **MySQL**, and **Apache Superset**.
This project reflects real-world data engineering workflows with complete automation, orchestration, and reporting â€” all running **without manual intervention** once deployed.

---

### ğŸ“¸ Superset in Action
 Here's a glimpse of the production-style dashboard created with Apache Superset:
![RetailFlow Dashboard](images/screenshot.png)
---

## âš™ï¸ Pipeline Overview

```text
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚  Raw CSVs  â”‚
               â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
[Ingestion.py â€“ saves raw data to processed data folder]
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cleaned CSV is saved inside /processed  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Transform.ipynb â€“ cleanup, feature engineering  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data_Loading.ipynb â€“ saves data into MySQL database â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
           ğŸ“Š Superset Dashboards

```

## âœ… Key Features & Achievements
 - ğŸ”„ Ingestion â†’ Transformation â†’ Load pipeline â€” fully automated via Airflow

 - â±ï¸ Scheduled DAGs using Airflow to run daily without manual trigger

 - ğŸ³ Containerized all services using Docker & Docker Compose

 - ğŸ—ƒï¸ Data stored in MySQL, accessible across containers

 - ğŸ“ˆ Designed insightful dashboards in Superset:

 - Time-series trends of daily/monthly sales

 - Product category performance

 - Gender-wise sales distribution (pie chart)

## ğŸ’¡ Real-world issues solved:

- Docker volume sync issues

- Multi-container networking (Airflow â†” MySQL â†” Superset)

- Path handling across host & container

- ğŸ“ Scalable folder structure with raw/processed separation

---
## ğŸ›  Tech Stack

| Layer            | Tools Used                        |
|------------------|-----------------------------------|
| Orchestration    | Apache Airflow                    |
| Scripting        | Python                            |
| Data Storage     | MySQL                             |
| Reporting        | Apache Superset                   |
| Containerization | Docker + Docker Compose           |
| Notebook Runner  | Papermill                         |

---

## ğŸ§ª How It Works
1. Data Ingestion: Ingests daily CSVs based on date naming convention
2. Transformation: Jupyter Notebook with business logic runs via Airflow
3. Transformed data pushed to MySQL
4. Visualization: Superset reads directly from MySQL for reporting
---

## ğŸ“ Dataset Information
- Dataset Name: Retail Sales Dataset â€“ Unveiling Retail Trends
- Source: Kaggle
- Rows: 62 (sample rows for testing)
- Included in data/raw_data/transactions_sample.csv for demo/testing purposes
---

##  ğŸ“¦ Folder Structure

```
RetailFlow/
â”œâ”€â”€ dags/                   # Airflow DAGs
â”œâ”€â”€ scripts/                # Python ingestion script
â”œâ”€â”€ notebooks/              # notebooks (transform & load)
â”‚   â””â”€â”€ transform_executed_notebooks_by_date
|   â””â”€â”€ data_loading_executed_notebooks_by_date
|   â””â”€â”€ transform.ipynb
|   â””â”€â”€ data_loading.ipynb
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_data/           
â”‚   â””â”€â”€ processed_data/
|   â””â”€â”€ transformed/
|   â””â”€â”€ aggregated/
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
``` 
---

## ğŸ”§ How to Run
```
# Clone and go to project
git clone https://github.com/mohdgeelani/RetailFlow.git
Go to project root folder then move to airflow_docker
cd airflow_docker

# Build and start all services
docker-compose up --build
```
### Then visit:

- Airflow: http://localhost:8080
- Superset: http://localhost:8088
- Default creds: admin/admin
---

## ğŸš€ Future Enhancements
 - Integrate AWS S3 or LocalStack for cloud-based data storage
 - Add alerts (e.g., Slack/email) on DAG failure
 - Include data validation using Great Expectations
 - Automate Superset dashboard refresh via API
 - Add unit tests for scripts/notebooks
---

## ğŸ’¬ Final Words

**RetailFlow** is more than just a project â€” it's a hands-on journey through real-world data engineering tasks such as data ingestion, transformation, data loading, orchestration, and containerization. 

Designed to be scalable, modular, and production-ready, this project showcases practical skills that align well with roles in **Data Engineering**, **Backend Development**, and **DevOps**.

Whether you're exploring the codebase or evaluating it as a portfolio piece, we hope RetailFlow offers meaningful insights and inspiration.


